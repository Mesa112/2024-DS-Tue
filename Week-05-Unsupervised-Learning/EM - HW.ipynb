{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall 2024 Data Science Track: Week 5 - Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages, Packages, Packages!\n",
    "\n",
    "Import *all* the things here! You need: `matplotlib`, `networkx`, `numpy`, and `pandas`â€•and also `ast.literal_eval` to correctly deserialize two columns in the `rules.tsv.xz` file.\n",
    "\n",
    "If you got more stuff you want to use, add them here too. ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donâ€™t worry about this. This is needed to interpret the Python code that is embedded in the data set. You only need it literally in the very next code cell and nowhere else. \n",
    "from ast import literal_eval\n",
    "\n",
    "# The rest is just the stuff from the lecture.\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "With the packages out of the way, now you will be working with the Instacart association rules data set, mined from the [Instacart Market Basket Analysis data set](https://www.kaggle.com/c/instacart-market-basket-analysis/data) on Kaggle. [The script](https://github.com/LiKenun/shopping-assistant/blob/main/api/preprocess_instacart_market_basket_analysis_data.py) that does it and the instructions to run it can be found in my [Shopping Assistant Project](https://github.com/LiKenun/shopping-assistant) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "This code has already been pre-written, simply because there are a few quirks which require converters to ensure the correct deserialization of some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     consequent_item  transaction_count  \\\n",
      "0  'Total 2% with Raspberry Pomegranate Lowfat Gr...            3346083   \n",
      "1  'Total 2% Lowfat Greek Strained Yogurt With Bl...            3346083   \n",
      "2  'Total 0% with Honey Nonfat Greek Strained Yog...            3346083   \n",
      "3                        'Total 0% Raspberry Yogurt'            3346083   \n",
      "4                              'Pineapple Yogurt 2%'            3346083   \n",
      "\n",
      "   item_set_count  antecedent_count  consequent_count  \\\n",
      "0             101               123               128   \n",
      "1             101               128               123   \n",
      "2             101               123               128   \n",
      "3             101               123               128   \n",
      "4             101               128               123   \n",
      "\n",
      "                                    antecedent_items  \n",
      "0  [\"Fat Free Blueberry Yogurt\", \"Pineapple Yogur...  \n",
      "1  [\"Fat Free Strawberry Yogurt\", \"Total 0% Raspb...  \n",
      "2  [\"Fat Free Blueberry Yogurt\", \"Pineapple Yogur...  \n",
      "3  [\"Fat Free Blueberry Yogurt\", \"Pineapple Yogur...  \n",
      "4  [\"Fat Free Strawberry Yogurt\", \"Total 0% Raspb...  \n"
     ]
    }
   ],
   "source": [
    "rules_data_path = '/Users/estebanm/Desktop/2024-DS-Tue/Week-05-Unsupervised-Learning/data/rules.tsv.xz'\n",
    "\n",
    "\n",
    "df_rules = pd.read_csv(rules_data_path, sep='\\t', low_memory=True)\n",
    "print(df_rules.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just *how* many rules were just loadedâ€½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column names: ['consequent_item', 'transaction_count', 'item_set_count', 'antecedent_count', 'consequent_count', 'antecedent_items']\n",
      "Number of rules:  1048575\n"
     ]
    }
   ],
   "source": [
    "# Show the list of column names and the number of rules.\n",
    "\n",
    "print(\"column names:\", df_rules.columns.to_list())\n",
    "\n",
    "print(\"Number of rules: \", len(df_rules) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Compute the support, confidence, and lift of each rule.\n",
    "\n",
    "* The ruleâ€™s *support* tells you how frequently the set of items appears in the dataset. Itâ€™s important to prune infrequent sets from further consideration.\n",
    "    * The simple definition: $$P(A \\cap B)$$\n",
    "    * `= item_set_count / transaction_count`\n",
    "* The ruleâ€™s *confidence* tells you how often a the rule is true. Divide the support for the set of items by the support for just the antecedents. Rules which are not true very often are also pruned.\n",
    "    * The simple definition: $$\\frac{P(A \\cap B)}{P(A)}$$\n",
    "    * `= item_set_count / transaction_count / (antecedent_count / transaction_count)`\n",
    "    * `= item_set_count / antecedent_count`\n",
    "* The ruleâ€™s *lift* tells you how much more likely the consequent is, given the antecedents, compared to its baseline probability. Divide the support for the set of items by both the support of the antecedents and consequent. Equivalently, divide the confidence by the support of the consequent.\n",
    "    * The simple definition: $$\\frac{P(A \\cap B)}{P(A) \\cdot P(B)}$$\n",
    "    * `= item_set_count / transaction_count / (antecedent_count / transaction_count * (consequent_count / transaction_count))`\n",
    "    * `= item_set_count / antecedent_count / (consequent_count / transaction_count)`\n",
    "    * `= item_set_count * transaction_count / (antecedent_count * consequent_count)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support  confidence          lift\n",
      "0   0.000030    0.821138  21465.598514\n",
      "1   0.000030    0.789062  21465.598514\n",
      "2   0.000030    0.821138  21465.598514\n",
      "3   0.000030    0.821138  21465.598514\n",
      "4   0.000030    0.789062  21465.598514\n",
      "5   0.000030    0.821138  21465.598514\n",
      "6   0.000030    0.789062  21465.598514\n",
      "7   0.000030    0.687075  19649.653061\n",
      "8   0.000030    0.687075  19649.653061\n",
      "9   0.000030    0.863248  19649.653061\n",
      "10  0.000030    0.863248  19649.653061\n",
      "11  0.000030    0.687075  19649.653061\n",
      "12  0.000030    0.687075  19649.653061\n",
      "13  0.000030    0.863248  19649.653061\n",
      "14  0.000030    0.863248  19649.653061\n",
      "15  0.000030    0.848739  19585.881368\n",
      "16  0.000030    0.696552  19585.881368\n",
      "17  0.000030    0.848739  19585.881368\n",
      "18  0.000030    0.696552  19585.881368\n",
      "19  0.000030    0.696552  19585.881368\n",
      "20  0.000030    0.848739  19585.881368\n",
      "21  0.000030    0.696552  19585.881368\n",
      "22  0.000030    0.765152  19250.078776\n",
      "23  0.000030    0.765152  19250.078776\n",
      "24  0.000030    0.759398  19250.078776\n",
      "25  0.000030    0.765152  19250.078776\n",
      "26  0.000030    0.759398  19250.078776\n",
      "27  0.000030    0.759398  19250.078776\n",
      "28  0.000030    0.765152  19250.078776\n",
      "29  0.000030    0.759398  19250.078776\n",
      "30  0.000031    0.798450  19220.709888\n",
      "31  0.000031    0.741007  19220.709888\n",
      "32  0.000031    0.741007  19220.709888\n",
      "33  0.000031    0.741007  19220.709888\n",
      "34  0.000031    0.798450  19220.709888\n",
      "35  0.000031    0.741007  19220.709888\n",
      "36  0.000031    0.741007  19220.709888\n",
      "37  0.000031    0.798450  19220.709888\n",
      "38  0.000031    0.798450  19220.709888\n",
      "39  0.000031    0.715278  19147.030500\n",
      "40  0.000031    0.715278  19147.030500\n",
      "41  0.000031    0.824000  19147.030500\n",
      "42  0.000031    0.715278  19147.030500\n",
      "43  0.000031    0.824000  19147.030500\n",
      "44  0.000031    0.715278  19147.030500\n",
      "45  0.000031    0.715278  19147.030500\n",
      "46  0.000031    0.824000  19147.030500\n",
      "47  0.000031    0.824000  19147.030500\n",
      "48  0.000030    0.829268  19136.555425\n",
      "49  0.000030    0.703448  19136.555425\n"
     ]
    }
   ],
   "source": [
    "# Add new columns support, confidence, and lift to df_rules. And show the first 50 rules.\n",
    "df_rules['support'] = df_rules['item_set_count'] / df_rules['transaction_count']\n",
    "\n",
    "# Step 2: Compute confidence\n",
    "df_rules['confidence'] = df_rules['item_set_count'] / df_rules['antecedent_count']\n",
    "\n",
    "# Step 3: Compute lift\n",
    "df_rules['lift'] = (df_rules['item_set_count'] * df_rules['transaction_count']) / (df_rules['antecedent_count'] * df_rules['consequent_count'])\n",
    "\n",
    "# Display the first 50 rows with the new metrics\n",
    "print(df_rules[['support', 'confidence', 'lift']].head(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yogurts have got some insane lift (*over 9,000*). Why do you think that might be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Write your answer here.)*\n",
    "\n",
    "The extremely high lift for yogurts, \"over 9,000,\" shows us that yogurt is disproportionately purchased alongside certain other items, far more than expected by chance. This could be due to frequent pairings with items like granola, making yogurt and these items highly associated. Additionally, if yogurt has low standalone support meaning almost never show by itself in transactionsâ€”but frequently occurs with specific items, it would result in a high lift value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     antecedent_items  \\\n",
      "0   [\"Fat Free Blueberry Yogurt\", \"Pineapple Yogur...   \n",
      "4   [\"Fat Free Strawberry Yogurt\", \"Total 0% Raspb...   \n",
      "6   [\"Fat Free Strawberry Yogurt\", \"Total 0% Raspb...   \n",
      "5   [\"Fat Free Blueberry Yogurt\", \"Pineapple Yogur...   \n",
      "1   [\"Fat Free Strawberry Yogurt\", \"Total 0% Raspb...   \n",
      "3   [\"Fat Free Blueberry Yogurt\", \"Pineapple Yogur...   \n",
      "2   [\"Fat Free Blueberry Yogurt\", \"Pineapple Yogur...   \n",
      "7   [\"Blackberry Yogurt\", \"Fat Free Strawberry Yog...   \n",
      "12  [\"Blackberry Yogurt\", \"Fat Free Strawberry Yog...   \n",
      "14  [\"Pineapple Yogurt 2%\", \"Total 0% Raspberry Yo...   \n",
      "\n",
      "                                      consequent_item  support  confidence  \\\n",
      "0   'Total 2% with Raspberry Pomegranate Lowfat Gr...  0.00003    0.821138   \n",
      "4                               'Pineapple Yogurt 2%'  0.00003    0.789062   \n",
      "6                         'Fat Free Blueberry Yogurt'  0.00003    0.789062   \n",
      "5                        'Fat Free Strawberry Yogurt'  0.00003    0.821138   \n",
      "1   'Total 2% Lowfat Greek Strained Yogurt With Bl...  0.00003    0.789062   \n",
      "3                         'Total 0% Raspberry Yogurt'  0.00003    0.821138   \n",
      "2   'Total 0% with Honey Nonfat Greek Strained Yog...  0.00003    0.821138   \n",
      "7   'Total 2% with Raspberry Pomegranate Lowfat Gr...  0.00003    0.687075   \n",
      "12                              'Pineapple Yogurt 2%'  0.00003    0.687075   \n",
      "14                                'Blackberry Yogurt'  0.00003    0.863248   \n",
      "\n",
      "            lift  \n",
      "0   21465.598514  \n",
      "4   21465.598514  \n",
      "6   21465.598514  \n",
      "5   21465.598514  \n",
      "1   21465.598514  \n",
      "3   21465.598514  \n",
      "2   21465.598514  \n",
      "7   19649.653061  \n",
      "12  19649.653061  \n",
      "14  19649.653061  \n"
     ]
    }
   ],
   "source": [
    "# Query rules where yogurt appears as the antecedent or consequent\n",
    "#first we create the yogurt rules to continue investigating\n",
    "yogurt_rules = df_rules[(df_rules['antecedent_items'].str.contains('yogurt', case=False, na=False)) | \n",
    "                        (df_rules['consequent_item'].str.contains('yogurt', case=False, na=False))]\n",
    "\n",
    "yogurt_rules.loc[:, 'support'] = yogurt_rules['item_set_count'] / yogurt_rules['transaction_count']\n",
    "yogurt_rules.loc[:, 'confidence'] = yogurt_rules['item_set_count'] / yogurt_rules['antecedent_count']\n",
    "yogurt_rules.loc[:, 'lift'] = (yogurt_rules['item_set_count'] * yogurt_rules['transaction_count']) / (yogurt_rules['antecedent_count'] * yogurt_rules['consequent_count'])\n",
    "\n",
    "\n",
    "sorted_yogurt_rules = yogurt_rules.sort_values(by='lift', ascending=False)\n",
    "print(sorted_yogurt_rules[['antecedent_items', 'consequent_item', 'support', 'confidence', 'lift']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Visualization for Consequents with Single Antecedents\n",
    "\n",
    "Letâ€™s now visualize a small subset of 1,000,000+ rules. First, filter the rule set for the following to whittle it down to something more manageable:\n",
    "\n",
    "1. The rule must have exactly `1` antecedent item. (There should be 38,684 such rules.)\n",
    "2. The lift must be between `5` and `20`. (There should be 1,596 such rules, including the prior criterion.)\n",
    "3. Either the antecedent or consequent of the rule must contain `'Hummus'`, but not both. (This should get you down to 26 rules.)\n",
    "    * Convert the antecedents `list`-typed column to a `str`-typed column (`antecedent_item`) since there will only be a single antecedent in the subset.\n",
    "    * Replace any item containing `'Hummus'` to just `'Hummus'`. This will make the visualization more readable later.\n",
    "\n",
    "Hint: your code may run more efficiently if you re-order certain processing steps.\n",
    "\n",
    "Assign the subset to `df_rules_subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define df_rules_subset.\n",
    "\n",
    "df_rules_subset = # Something goes here. You can repeat this for as many steps as necessary. Technically, you can do it all in one shot. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a network `graph_rules_subset` from the association rules subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph_rules_subset, add the graphâ€™s edges, and plot it. You may need a large figure size, smaller node size, and smaller font size.\n",
    "\n",
    "graph_rules_subset = nx.MultiDiGraph()\n",
    "graph_rules_subset.add_edges_from(\n",
    "    # Something goes here.\n",
    ")\n",
    "\n",
    "# Then render the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you tell about people who buy hummus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Write your answer here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Prediction\n",
    "\n",
    "Given that the basket of items contains the following items, use the full set of association rules to predict the next 20 most likely items (consequents) that the person will add to the basket in descending order of lift:\n",
    "\n",
    "* `'Orange Bell Pepper'`\n",
    "* `'Organic Red Bell Pepper'`\n",
    "\n",
    "Hint: a single item in the basket may be a better predictor of some consequents than both items considered together. You must consider both or either, but not neither."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket = {'Orange Bell Pepper', 'Organic Red Bell Pepper'}\n",
    "\n",
    "df_rules[# A few conditions go here which takes care of all the following cases:\n",
    "         # * Just Orange Bell Pepper\n",
    "         # * Just Organic Red Bell Pepper\n",
    "         # * Both Orange Bell Pepper and Organic Red Bell Pepper\n",
    "         # You can do it using just 2 conditions. :)\n",
    "         ] \\\n",
    "        .sort_values('lift', ascending=False) \\\n",
    "        .head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other Interesting Findings\n",
    "\n",
    "Find and share something else interesting about these association rules. It can be a graph, table, or some other format that illustrates your point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
